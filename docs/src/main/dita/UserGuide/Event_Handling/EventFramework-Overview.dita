<?xml version="1.0" encoding="UTF-8"?>
<!--

    Copyright 2017 StreamSets Inc.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_cph_5h4_lx">
    <title>Dataflow Triggers Overview</title>
    <conbody>
        <p><indexterm>dataflow triggers<indexterm>overview</indexterm></indexterm><indexterm>event
                    framework<indexterm>overview</indexterm></indexterm><term>Dataflow
                triggers</term> are instructions for the event framework to kick off tasks in
            response to events that occur in the pipeline. For example, you can use dataflow
            triggers to start a MapReduce job after the pipeline writes a file to HDFS. Or you might
            use a dataflow trigger to stop a pipeline after the JDBC Query Consumer origin processes
            all available data. </p>
        <p>The event framework consist of the following components:<dl>
                <dlentry>
                    <dt>event generation</dt>
                    <dd>The event framework generates pipeline-related events and stage-related
                        events. The framework generates pipeline events only when the pipeline
                        starts and stops. The framework generates stage events when specific
                        stage-related actions take place. The action that generates an event differs
                        from stage to stage and is related to how the stage processes data. </dd>
                    <dd>For example, the Hive Metastore destination updates the Hive metastore, so
                        it generates events each time it changes the metastore. In contrast, the
                        Hadoop FS destination writes files to HDFS, so it generates events each time
                        it closes a file. </dd>
                    <dd>Events produce <term>event records</term>. Pipeline-related event records
                        are passed immediately to the specified event consumer. Stage-related event
                        records are passed through the pipeline in an <term>event stream</term>. </dd>
                </dlentry>
                <dlentry>
                    <dt>task execution</dt>
                    <dd>To trigger a task, you need an <term>executor</term>. Executor stages
                        perform tasks in <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        /> or external systems. Each time an executor receives an event, it performs
                        the specified task.</dd>
                    <dd>For example, the Hive Query executor runs user-defined Hive or Impala
                        queries each time it receives an event, and the MapReduce executor triggers
                        a MapReduce job when it receives events. Within <ph
                            conref="../Reusable_Content/ReusablePhrases.dita#concept_vhs_5tz_xp/pName-long"
                        />, the Pipeline Finisher executor stops a pipeline upon receiving an event,
                        transitioning the pipeline to a Finished state. </dd>
                </dlentry>
                <dlentry>
                    <dt>event storage</dt>
                    <dd>To store event information, pass the event to a destination. The destination
                        writes the event records to the destination system, just like any other
                        data.</dd>
                    <dd>For example, you might store event records to keep an audit trail of the
                        files that the pipeline origin reads. </dd>
                </dlentry>
            </dl></p>
    </conbody>
</concept>
