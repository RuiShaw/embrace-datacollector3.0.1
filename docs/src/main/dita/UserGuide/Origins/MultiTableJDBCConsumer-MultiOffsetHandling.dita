<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_d52_4l1_q1b">
 <title>Multiple Offset Value Handling</title>
 <conbody>
  <p><indexterm>JDBC Multitable Consumer origin<indexterm>multiple offset
                values</indexterm></indexterm>When processing partitions, JDBC Multitable Consumer
            origin allows processing multiple records with the same offset value. For example, the
            origin can process multiple records with the same timestamp in a transaction_date offset
            column.</p>
        <p>
            <note type="warning">When processing multiple records with the same offset value,
                records can be dropped if you stop the pipeline when the origin is processing a
                series of records with the same offset value.</note>
        </p>
        <p>When you stop the pipeline as the origin is processing a series of records with the same
            offset value, the origin notes the offset. Then, when you restart the pipeline, it
            starts with a record with the next logical offset value, skipping any unprocessed
            records that use the same last-saved offset. </p>
        <p>For example, say you specified a datetime column as a user-defined offset column, and
            five records in the table share the same datetime value. Now say you happen to stop the
            pipeline after it processes the second record. The pipeline stores the datetime value as
            the offset where it stopped. When you restart the pipeline, processing begins with the
            next datetime value, skipping the three unprocessed records with the last-saved offset
            value. </p>
 </conbody>
</concept>
