<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
  
      http://www.apache.org/licenses/LICENSE-2.0
      
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_sjh_213_v1b">
 <title>Multiple Partitions, No Table Processing</title>
 <conbody>
  <p>Say you have table A, B, and C and all three tables are loaded up with lots of data to process.
            Tables A and B are configured with a maximum of 3 active partitions. And since table C
            has the largest volume of data, you allow an unlimited number of partitions. Again,
            let's use the alphabetical initial table ordering. </p>
        <p>When you start the pipeline, each table is queued up with the maximum number of active
            partitions. And for table C, that means double the number of threads for the pipeline.
            So if we configure the pipeline for 4 threads, table C can have up to 8 partitions in
            the queue at any given time. So the initial queue looks like
            this:<codeblock>A1  A2  A3  B1  B2  B3  C1  C2  C3  C4  C5  C6  C7  C8 </codeblock>A
            partition remains in the queue until the origin confirms that there is no more data in
            the partition. When a thread becomes available, it creates a set of batches from the
            first partition of the first table in the queue. The number of batches is based on the
            Batches from Result Set property. The order of tables and partitions in the queue
            depends on how you define the Per Batch Strategy, as follows:<dl>
                <dlentry>
                    <dt>Process All Available Rows in the Table</dt>
                    <dd>When processing partitions, this batch strategy retains the original order
                        of the queue, but rotates through the partitions as each thread processes a
                        set of batches. <note>In practice, this means that rows from subsequent
                            tables can be processed before a previous table is completed, since
                            available threads continue to pick up partitions from the
                        queue.</note></dd>
                    <dd>For example, the four threads start processing on the first four partitions
                        in the queue: A1, A2, A3, and B1. This puts B2 at the front of the queue,
                        ready for the next available thread. And since the four partitions being
                        processed have additional data to process, they go to the back of the queue.
                        So processing of table B data begins before table A is fully processed. </dd>
                    <dd>
                        <p>The rest of the partitions remain in the original order as follows:</p>
                        <codeblock>B2  B3  C1  C2  C3  C4  C5  C6  C7  C8  A1  A2  A3  B1</codeblock>
                    </dd>
                    <dd>After the four threads process another four sets of batches, the queue looks
                        like
                        this:<codeblock>C3  C4  C5  C6  C7  C8  A1  A2  A3  B1  B2  B3  C1  C2</codeblock></dd>
                </dlentry>
                <dlentry>
                    <dt>Switch Tables</dt>
                    <dd>When processing partitions, this batch strategy forces all subsequent,
                        consecutive partitions from the same table to the end of the queue each time
                        a thread processes a set of batches from a partition.</dd>
                    <dd>
                        <p>Let's start again with the initial batch order:</p>
                        <codeblock>A1  A2  A3  B1  B2  B3  C1  C2  C3  C4  C5  C6  C7  C8 </codeblock>
                    </dd>
                    <dd>When a thread processes a set of batches from A1, it pushes the rest of the
                        table A partitions to the end of the queue. This queues up the next table,
                        table B, for processing. And since A1 still contains data, it takes the last
                        spot, as follows:
                        <codeblock>B1  B2  B3  C1  C2  C3  C4  C5  C6  C7  C8  A2  A3  A1</codeblock>As
                        the second thread processes a set of batches from B1, the other B partitions
                        are sent to the back, and since B1 still contains data, it takes the last
                        spot as follows:
                        <codeblock>C1  C2  C3  C4  C5  C6  C7  C8  A2  A3  A1  B2  B3  B1</codeblock>And
                        as the third thread takes a set of batches from C1, the rest of the C
                        partitions are pushed to the back, so the queue looks like
                        this:<codeblock>A2  A3  A1  B2  B3  B1  C2  C3  C4  C5  C6  C7  C8  C1</codeblock></dd>
                </dlentry>
            </dl></p>
 </conbody>
</concept>
